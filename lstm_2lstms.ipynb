{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "#\n",
    "from sklearn.cross_validation import train_test_split\n",
    "#\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "import torchvision.models as models\n",
    "#\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.load(\"embedding_matrix.npy\")\n",
    "x1 = np.load(\"x1.npy\")\n",
    "x2 = np.load(\"x2.npy\")\n",
    "y = np.load(\"y.npy\")\n",
    "y = np_utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAMPLES = 10#len(y) ### 404224 is divisible by 128, the batch size\n",
    "USE_CUDA = False\n",
    "MAX_EPOCH = 1\n",
    "PRINT_LOSS_AT_EVERY = 1\n",
    "BATCH_SIZE = 1\n",
    "EARLY_STOPPING_CRITERIA = \"loss\" # loss or accuracy\n",
    "EARLY_STOPPING = 5\n",
    "HIDDEN_DIM = 200\n",
    "EMBEDDING_DIM = 300\n",
    "NUM_CLASS = 2\n",
    "NLAYERS = 1\n",
    "DENSE_NEURONS = 125\n",
    "#time.sleep(9000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, NLAYERS, batch_first=True, dropout=0.2)\n",
    "        self.hidden2dense = nn.Linear(hidden_dim*2, DENSE_NEURONS)\n",
    "        self.dense2class = nn.Linear(DENSE_NEURONS, NUM_CLASS)\n",
    "        self.simoidlayer = nn.Sigmoid()\n",
    "        self.batchnorm1 = nn.BatchNorm1d(hidden_dim*2)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(DENSE_NEURONS)\n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    " \n",
    "    def init_hidden(self):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (Variable(weight.new(NLAYERS, BATCH_SIZE, self.hidden_dim).zero_()), \n",
    "                Variable(weight.new(NLAYERS, BATCH_SIZE, self.hidden_dim).zero_()))    \n",
    "\n",
    "    def forward(self, x1, x2, hidden1, hidden2):\n",
    "        lstm_out1, hidden1 = self.lstm(self.drop(x1), hidden1)\n",
    "        lstm_out2, hidden2 = self.lstm(self.drop(x2), hidden2)\n",
    "        \n",
    "        #take the hiddenvalues from the last element in the sequence\n",
    "        #x1 = self.drop(lstm_out1[:,-1,:])\n",
    "        #x2 = self.drop(lstm_out2[:,-1,:])\n",
    "        x1 = self.drop(torch.mean(lstm_out1,1))\n",
    "        x2 = self.drop(torch.mean(lstm_out2,1))\n",
    "        x = torch.cat((x1, x2),1) # concatenation doubles dimension\n",
    "        #x = self.batchnorm1(x)\n",
    "        #x = self.drop(x)        \n",
    "        #\n",
    "        #x = lstm_out.select(1, len(sentence)-1).contiguous() # batchfirst = True\n",
    "        x = x.view(-1, self.hidden_dim*2)\n",
    "        x = F.relu(self.hidden2dense(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.drop(x)        \n",
    "        x = self.dense2class(x)\n",
    "        x = self.simoidlayer(x)\n",
    "        return x\n",
    "    \n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "    if type(h) == Variable:\n",
    "        return Variable(h.data)\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples =  8\n",
      "Number of validation samples =  2\n"
     ]
    }
   ],
   "source": [
    "# split into train/test\n",
    "X1_train, X1_valid, y_train, y_valid = train_test_split(x1[:SAMPLES], y[:SAMPLES], test_size=0.2, random_state=0)\n",
    "X2_train, X2_valid, _, _ = train_test_split(x2[:SAMPLES], y[:SAMPLES], test_size=0.2, random_state=0)\n",
    "print \"Number of training samples = \", len(y_train)\n",
    "print \"Number of validation samples = \", len(y_valid)\n",
    "\n",
    "if USE_CUDA == True:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    net.cuda()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.BCELoss()\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "\n",
    "trainfeatures1 = torch.from_numpy(X1_train).long()\n",
    "traintargets1 = torch.from_numpy(y_train).float()\n",
    "trainset1 = data_utils.TensorDataset(trainfeatures1, traintargets1)\n",
    "trainloader1 = data_utils.DataLoader(trainset1, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "trainfeatures2 = torch.from_numpy(X2_train).long()\n",
    "traintargets2 = torch.from_numpy(y_train).float()\n",
    "trainset2 = data_utils.TensorDataset(trainfeatures2, traintargets2)\n",
    "trainloader2 = data_utils.DataLoader(trainset2, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "validfeatures1 = torch.from_numpy(X1_valid).long()\n",
    "validtargets1 = torch.from_numpy(y_valid).float()\n",
    "validset1 = data_utils.TensorDataset(validfeatures1, validtargets1)\n",
    "validloader1 = data_utils.DataLoader(validset1, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "validfeatures2 = torch.from_numpy(X2_valid).long()\n",
    "validtargets2 = torch.from_numpy(y_valid).float()\n",
    "validset2 = data_utils.TensorDataset(validfeatures2, validtargets2)\n",
    "validloader2 = data_utils.DataLoader(validset2, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "embedding = nn.Embedding(embedding_matrix.shape[0], 300)\n",
    "embedding.weight.data = torch.from_numpy(embedding_matrix).float()\n",
    "if USE_CUDA == True:\n",
    "    embedding.weight.data = torch.from_numpy(embedding_matrix).float().cuda()\n",
    "embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    hidden1 = net.init_hidden()\n",
    "    hidden2 = net.init_hidden()\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    train_loss = 0.0\n",
    "    for i, (data1,data2) in enumerate(zip(trainloader1, trainloader2), 0):\n",
    "        hidden1 = repackage_hidden(hidden1)\n",
    "        hidden2 = repackage_hidden(hidden2)\n",
    "        inputs1, _ = data1\n",
    "        inputs2, labels = data2\n",
    "        if USE_CUDA == True:\n",
    "            inputs1 = inputs1.cuda()\n",
    "            inputs2 = inputs2.cuda()\n",
    "            labels = labels.cuda()\n",
    "        labels = Variable(labels)\n",
    "        inputs1 = Variable(inputs1)\n",
    "        inputs1 = embedding(inputs1)\n",
    "        #\n",
    "        inputs2 = Variable(inputs2)\n",
    "        inputs2 = embedding(inputs2)\n",
    "        #\n",
    "        #inputs = torch.cat((inputs1, inputs2),1)\n",
    "        ###inputs = inputs.permute(1,0,2,)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs1, inputs1, hidden1, hidden2)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.data[0]\n",
    "        train_loss += loss.data[0]\n",
    "        if i % PRINT_LOSS_AT_EVERY == PRINT_LOSS_AT_EVERY-1: \n",
    "            print('[%d, %5d] Train loss: %.3f' % (epoch+1, i+1, running_loss / PRINT_LOSS_AT_EVERY))\n",
    "            running_loss = 0.0\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def valid():\n",
    "    hidden1 = net.init_hidden()\n",
    "    hidden2 = net.init_hidden()\n",
    "    net.eval()\n",
    "    running_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    correct = 0\n",
    "    for i, (data1,data2) in enumerate(zip(validloader1, validloader2), 0):\n",
    "        hidden1 = repackage_hidden(hidden1)\n",
    "        hidden2 = repackage_hidden(hidden2)\n",
    "        inputs1, _ = data1\n",
    "        inputs2, labels = data2\n",
    "        if USE_CUDA == True:\n",
    "            inputs1 = inputs1.cuda()\n",
    "            inputs2 = inputs2.cuda()\n",
    "            labels = labels.cuda()        \n",
    "        labels = Variable(labels)\n",
    "        inputs1 = Variable(inputs1, volatile=True)\n",
    "        inputs1 = embedding(inputs1)\n",
    "        #\n",
    "        inputs2 = Variable(inputs2, volatile=True)\n",
    "        inputs2 = embedding(inputs2)\n",
    "        #\n",
    "        #inputs = torch.cat((inputs1, inputs2),1)\n",
    "        ###inputs = inputs.permute(1,0,2)\n",
    "        #\n",
    "        outputs = net(inputs1, inputs1, hidden1, hidden2)\n",
    "        pred = outputs.data.max(1)[1] \n",
    "        correct += pred.eq(labels.data.max(1)[1].long()).cpu().sum()\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.data[0]\n",
    "        valid_loss += loss.data[0]\n",
    "        if i % PRINT_LOSS_AT_EVERY == PRINT_LOSS_AT_EVERY-1:\n",
    "            print('[%d, %5d] Valid loss: %.3f' % (epoch+1, i+1, running_loss / PRINT_LOSS_AT_EVERY))\n",
    "            running_loss = 0.0\n",
    "    return correct, valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started ....\n",
      "[1,     1] Train loss: 0.688\n",
      "[1,     2] Train loss: 0.700\n",
      "[1,     3] Train loss: 0.687\n",
      "[1,     4] Train loss: 0.686\n",
      "[1,     5] Train loss: 0.701\n",
      "[1,     6] Train loss: 0.684\n",
      "[1,     7] Train loss: 0.684\n",
      "[1,     8] Train loss: 0.683\n",
      "[1,     1] Valid loss: nan\n",
      "[1,     2] Valid loss: nan\n",
      "Validation set accuracy: 2/2 (100.00%)\n",
      "Delta between train-loss and valid-loss: nan\n",
      "\n",
      "It has been 3.66819095612 seconds since the training loop started\n",
      "Finished Training\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "valid_loss = []\n",
    "accuracy = 0.0\n",
    "delta_loss = 1e15\n",
    "early_stop = 0\n",
    "program_starts = time.time()\n",
    "print('Training started ....')    \n",
    "for epoch in range(MAX_EPOCH):\n",
    "    train_loss.append(train()/len(trainloader1))\n",
    "    correct, _ = valid()\n",
    "    valid_loss.append(_/len(validloader1))\n",
    "    #\n",
    "    plt.plot(train_loss, label=\"Train loss\")\n",
    "    plt.plot(valid_loss, label=\"Valid loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"train-valid_loss.eps\")\n",
    "    plt.clf()\n",
    "    #\n",
    "    print('Validation set accuracy: {}/{} ({:.2f}%)'.format(correct, len(validloader1)*BATCH_SIZE, \n",
    "                                                            100. * correct / (len(validloader1)*BATCH_SIZE)))\n",
    "    print('Delta between train-loss and valid-loss: {:0.4f}\\n'.format(abs(train_loss[epoch]-valid_loss[epoch])))    \n",
    "    if EARLY_STOPPING_CRITERIA == \"accuracy\":\n",
    "        if accuracy < (float(correct) / (len(validloader1)*BATCH_SIZE)):\n",
    "            torch.save(net, \"net_best-model.pth\")\n",
    "            accuracy = float(correct) / (len(validloader1)*BATCH_SIZE)\n",
    "            print \"This is the current best model based on accuracy. Updated!\"\n",
    "            early_stop = 0\n",
    "    elif EARLY_STOPPING_CRITERIA == \"loss\":\n",
    "        if abs(train_loss[epoch]-valid_loss[epoch]) < delta_loss:\n",
    "            torch.save(net, \"net_best-model.pth\")\n",
    "            delta_loss = abs(train_loss[epoch]-valid_loss[epoch])\n",
    "            print \"This is the current best model based on loss. Updated!\"\n",
    "            early_stop = 0        \n",
    "    early_stop += 1\n",
    "    if early_stop == EARLY_STOPPING:\n",
    "        if EARLY_STOPPING_CRITERIA == \"accuracy\":\n",
    "            print \"Validation accuracy is not better than {:.2f}%  over the last {} epochs. Training stopped after epoch {}\".format(accuracy*100., EARLY_STOPPING, epoch+1)\n",
    "            break\n",
    "        elif EARLY_STOPPING_CRITERIA == \"loss\":\n",
    "            print \"Delta between train-loss and valid-loss is not less than {:0.4f}  over the last {} epochs. Training stopped after epoch {}\".format(delta_loss, EARLY_STOPPING, epoch+1)\n",
    "            break        \n",
    "now = time.time()\n",
    "print(\"It has been {0} seconds since the training loop started\".format(now - program_starts))\n",
    "torch.save(net, \"net.pth\")\n",
    "print('Finished Training\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = torch.rand(5,10,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 15])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(x,1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
